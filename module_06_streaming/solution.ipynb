{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "def json_serializer(data):\n",
    "    return json.dumps(data).encode('utf-8')\n",
    "\n",
    "server = 'localhost:9092'\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers = [server],\n",
    "    value_serializer = json_serializer\n",
    ")\n",
    "\n",
    "producer.bootstrap_connected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 'green-trips' already exists.\n"
     ]
    }
   ],
   "source": [
    "# Creating the topic green-trips\n",
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "\n",
    "admin_client = KafkaAdminClient(\n",
    "    bootstrap_servers=[server],\n",
    "    client_id='test'\n",
    ")\n",
    "\n",
    "topic_name = 'green-trips'\n",
    "\n",
    "# Check if the topic exists\n",
    "if topic_name in admin_client.list_topics():\n",
    "    print(f\"Topic '{topic_name}' already exists.\")\n",
    "else:\n",
    "    # Create the topic\n",
    "    topic_list = [NewTopic(name=topic_name, num_partitions=1, replication_factor=1)]\n",
    "    admin_client.create_topics(new_topics=topic_list, validate_only=False)\n",
    "    print(f\"Topic '{topic_name}' created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics in Kafka: ['green-data', '__consumer_offsets', 'green-trips']\n"
     ]
    }
   ],
   "source": [
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "admin_client = KafkaAdminClient(\n",
    "    bootstrap_servers=[server],\n",
    "    client_id='test'\n",
    ")\n",
    "# Show all topics in kafka\n",
    "topics = admin_client.list_topics()\n",
    "print(\"Topics in Kafka:\", topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from kafka import KafkaProducer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()  # Start time\n",
    "\n",
    "    # Create a Kafka producer\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers=server,\n",
    "        value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    "    )\n",
    "\n",
    "    csv_file = 'green_tripdata_2019-10.csv'  # change to your CSV file path if needed\n",
    "\n",
    "    with open(csv_file, 'r', newline='', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        total_rows = sum(1 for row in reader)\n",
    "        file.seek(0)  # Reset file pointer to the beginning\n",
    "        next(reader)  # Skip header row\n",
    "\n",
    "        for row in tqdm(reader, total=total_rows, desc=\"Sending data to Kafka\"):\n",
    "            # Each row will be a dictionary keyed by the CSV headers\n",
    "            # Send data to Kafka topic \"green-data\"\n",
    "            producer.send('green-trips', value=row)\n",
    "\n",
    "    # Make sure any remaining messages are delivered\n",
    "    producer.flush()\n",
    "    producer.close()\n",
    "\n",
    "    end_time = time.time()  # End time\n",
    "    logging.info(f\"Time taken: {round(end_time - start_time)} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from kafka import KafkaConsumer\n",
    "\n",
    "# # Create a Kafka consumer\n",
    "# consumer = KafkaConsumer(\n",
    "#     'green-trips',\n",
    "#     bootstrap_servers=[server],\n",
    "#     auto_offset_reset='earliest',\n",
    "#     enable_auto_commit=True,\n",
    "#     group_id='my-group',\n",
    "#     value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n",
    "# )\n",
    "\n",
    "# # Read data from the topic\n",
    "# for message in consumer:\n",
    "#     print(f\"Received message: {message.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.table import EnvironmentSettings, StreamTableEnvironment\n",
    "from pyflink.common.watermark_strategy import WatermarkStrategy\n",
    "from pyflink.common.time import Duration\n",
    "\n",
    "def create_events_aggregated_sink(t_env):\n",
    "    table_name = 'processed_events_aggregated'\n",
    "    sink_ddl = f\"\"\"\n",
    "        CREATE TABLE {table_name} (\n",
    "            window_start TIMESTAMP(3),\n",
    "            window_end TIMESTAMP(3),\n",
    "            PULocationID INT,\n",
    "            DOLocationID INT,\n",
    "            num_hits BIGINT,\n",
    "            PRIMARY KEY (window_start, PULocationID, DOLocationID) NOT ENFORCED\n",
    "        ) WITH (\n",
    "            'connector' = 'jdbc',\n",
    "            'url' = 'jdbc:postgresql://postgres:5432/postgres',\n",
    "            'table-name' = '{table_name}',\n",
    "            'username' = 'postgres',\n",
    "            'password' = 'postgres',\n",
    "            'driver' = 'org.postgresql.Driver'\n",
    "        );\n",
    "        \"\"\"\n",
    "    t_env.execute_sql(sink_ddl)\n",
    "    return table_name\n",
    "\n",
    "def create_events_source_kafka(t_env):\n",
    "    table_name = \"green_trips\"\n",
    "    source_ddl = f\"\"\"\n",
    "        CREATE TABLE {table_name} (\n",
    "            lpep_pickup_datetime TIMESTAMP(3),\n",
    "            lpep_dropoff_datetime TIMESTAMP(3),\n",
    "            PULocationID INT,\n",
    "            DOLocationID INT,\n",
    "            passenger_count INT,\n",
    "            trip_distance FLOAT,\n",
    "            tip_amount FLOAT,\n",
    "            event_watermark AS lpep_dropoff_datetime,\n",
    "            WATERMARK for event_watermark as event_watermark - INTERVAL '5' SECOND\n",
    "        ) WITH (\n",
    "            'connector' = 'kafka',\n",
    "            'properties.bootstrap.servers' = 'redpanda-1:29092',\n",
    "            'topic' = 'green-trips',\n",
    "            'scan.startup.mode' = 'earliest-offset',\n",
    "            'properties.auto.offset.reset' = 'earliest',\n",
    "            'format' = 'json'\n",
    "        );\n",
    "        \"\"\"\n",
    "    t_env.execute_sql(source_ddl)\n",
    "    return table_name\n",
    "\n",
    "def log_aggregation():\n",
    "    # Set up the execution environment\n",
    "    env = StreamExecutionEnvironment.get_execution_environment()\n",
    "    env.enable_checkpointing(10 * 1000)\n",
    "    env.set_parallelism(3)\n",
    "\n",
    "    # Set up the table environment\n",
    "    settings = EnvironmentSettings.new_instance().in_streaming_mode().build()\n",
    "    t_env = StreamTableEnvironment.create(env, environment_settings=settings)\n",
    "\n",
    "    try:\n",
    "        # Create Kafka table\n",
    "        source_table = create_events_source_kafka(t_env)\n",
    "        aggregated_table = create_events_aggregated_sink(t_env)\n",
    "\n",
    "        # Execute the SQL query\n",
    "        t_env.execute_sql(f\"\"\"\n",
    "        INSERT INTO {aggregated_table}\n",
    "        SELECT\n",
    "            window_start,\n",
    "            window_end,\n",
    "            PULocationID,\n",
    "            DOLocationID,\n",
    "            COUNT(*) AS num_hits\n",
    "        FROM TABLE(\n",
    "            SESSION(TABLE {source_table}, DESCRIPTOR(event_watermark), INTERVAL '5' MINUTE)\n",
    "        )\n",
    "        GROUP BY window_start, window_end, PULocationID, DOLocationID;\n",
    "        \"\"\").wait()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Writing records from Kafka to JDBC failed:\", str(e))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    log_aggregation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyflink-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
